[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tue-mps.github.io/author/mps-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mps-lab/","section":"authors","summary":"","tags":null,"title":"MPS Lab","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://tue-mps.github.io/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://tue-mps.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":["Internship"],"content":"Summary: During this project, you will improve an existing network for panoptic segmentation, to make it more accurate and efficient.\nType: Internship\nStarting date: September 2021\nSupervisor: Daan de Geus\nGeneral description:\nSelf-driving vehicles need situational awareness to be able to take appropriate actions. One way of gaining situational awareness is by applying scene understanding algorithms to cameras mounted on the vehicle. Panoptic segmentation is such a scene understanding task, which aims at recognizing all entities in a scene. Specifically, the goal is to predict, for each pixel, 1) a scene-class label (e.g., car, person, sky), 2) an instance id to distinguish between individual objects of countable classes (e.g., individual cars or persons).\nOver the past years, several deep neural networks have been developed for panoptic segmentation, with varying accuracies and efficiencies. Especially for self-driving vehicles, where real-time applicability is key, efficiency of neural networks is very important. For this purpose, the Fast Panoptic Segmentation Network (FPSNet) was introduced [1]. FPSNet achieves efficient panoptic segmentation by making use of a fast object detection backbone, and an attention mechanism that indicates the location of individual objects.\nHowever, recent work has outperformed FPSNet in terms of accuracy and efficiency, making use of various novel techniques. The goal of this internship is to leverage some of these techniques to improve the efficiency and accuracy of FPSNet.\nTask description: During this project, you will implement and evaluate several improvements to FPSNet. These improvements include:\n More supervision of objects attended by attention masks. More accurate attention masks, instead of using bell-shaped blobs. These could be generated by letting the network learn borderness of objects. More advanced panoptic head architecture. More efficient and accurate detection backbone. Further optimization of hyperparameters and learning strategy.  Prerequisites:\n Theoretical knowledge about deep neural networks for computer vision. Experience with implementing a deep neural network for computer vision.  Interested? Send an email to d.c.d.geus@tue.nl, containing:\n Brief motivation letter List of relevant courses and grades  References\n[1] D. de Geus, P. Meletis and G. Dubbelman, \u0026ldquo;Fast Panoptic Segmentation Network\u0026rdquo;, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1742-1749, 2020.\n","date":1625788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625788800,"objectID":"f89fcc70b25f3b1ee5916838c789aa4e","permalink":"https://tue-mps.github.io/post/2021-09-internship-panoptic-segmentation/","publishdate":"2021-07-09T00:00:00Z","relpermalink":"/post/2021-09-internship-panoptic-segmentation/","section":"post","summary":"Summary: During this project, you will improve an existing network for panoptic segmentation, to make it more accurate and efficient.\n","tags":null,"title":"Accuracy and Efficiency Improvements for Fast Panoptic Segmentation","type":"post"},{"authors":null,"categories":["Graduation Project"],"content":"Summary: During this project, you will \u0026hellip;\nType: Graduation project\nStarting date: September 2021\nSupervisor: TBD\nGeneral description:\nGeneral description here\nTask description: During this project, you will carry out several tasks\u0026hellip;\nPrerequisites:\n Theoretical knowledge about deep neural networks for computer vision. Experience with implementing a deep neural network for computer vision.  Interested? Send an email to (EMAIL HERE), containing:\n Brief motivation letter List of relevant courses and grades ","date":1625788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625788800,"objectID":"1202177f564f3ca1fe2ba91fdfe1ccd8","permalink":"https://tue-mps.github.io/post/2021-09-grad-project-title-here/","publishdate":"2021-07-09T00:00:00Z","relpermalink":"/post/2021-09-grad-project-title-here/","section":"post","summary":"Summary: During this project, you will \u0026hellip;\n","tags":null,"title":"Graduation project title","type":"post"},{"authors":["Test"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://tue-mps.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://tue-mps.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://tue-mps.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://tue-mps.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://tue-mps.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://tue-mps.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]