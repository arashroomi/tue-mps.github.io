<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mobile Perception Systems Lab</title><link>https://tue-mps.github.io/</link><atom:link href="https://tue-mps.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Mobile Perception Systems Lab</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://tue-mps.github.io/media/icon_hu9bffa56b6a2f6e08d28b87009a0bc873_1397_512x512_fill_lanczos_center_3.png</url><title>Mobile Perception Systems Lab</title><link>https://tue-mps.github.io/</link></image><item><title>Example Event</title><link>https://tue-mps.github.io/event/example/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://tue-mps.github.io/event/example/</guid><description>&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Accuracy and Efficiency Improvements for Fast Panoptic Segmentation</title><link>https://tue-mps.github.io/post/2021-09-internship-panoptic-segmentation/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/post/2021-09-internship-panoptic-segmentation/</guid><description>&lt;p>&lt;strong>Summary:&lt;/strong> During this project, you will improve an existing network for panoptic segmentation, to make it more accurate and efficient.&lt;/p>
&lt;p>&lt;strong>Type:&lt;/strong> Internship&lt;/p>
&lt;p>&lt;strong>Starting date:&lt;/strong> September 2021&lt;/p>
&lt;p>&lt;strong>Supervisor:&lt;/strong> Daan de Geus&lt;/p>
&lt;p>&lt;strong>General description:&lt;/strong>&lt;/p>
&lt;p>Self-driving vehicles need situational awareness to be able to take appropriate actions. One way of gaining situational awareness is by applying scene understanding algorithms to cameras mounted on the vehicle. Panoptic segmentation is such a scene understanding task, which aims at recognizing all entities in a scene. Specifically, the goal is to predict, for each pixel, 1) a scene-class label (&lt;em>e.g.&lt;/em>, car, person, sky), 2) an instance &lt;em>id&lt;/em> to distinguish between individual objects of countable classes (&lt;em>e.g.&lt;/em>, individual cars or persons).&lt;/p>
&lt;p>Over the past years, several deep neural networks have been developed for panoptic segmentation, with varying accuracies and efficiencies. Especially for self-driving vehicles, where real-time applicability is key, efficiency of neural networks is very important. For this purpose, the Fast Panoptic Segmentation Network (FPSNet) was introduced [1].
FPSNet achieves efficient panoptic segmentation by making use of a fast object detection backbone, and an attention mechanism that indicates the location of individual objects.&lt;/p>
&lt;p>However, recent work has outperformed FPSNet in terms of accuracy and efficiency, making use of various novel techniques. The goal of this internship is to leverage some of these techniques to improve the efficiency and accuracy of FPSNet.&lt;/p>
&lt;p>&lt;strong>Task description:&lt;/strong>
During this project, you will implement and evaluate several improvements to FPSNet. These improvements include:&lt;/p>
&lt;ul>
&lt;li>More supervision of objects attended by attention masks.&lt;/li>
&lt;li>More accurate attention masks, instead of using bell-shaped blobs. These could be generated by letting the network learn &lt;em>borderness&lt;/em> of objects.&lt;/li>
&lt;li>More advanced panoptic head architecture.&lt;/li>
&lt;li>More efficient and accurate detection backbone.&lt;/li>
&lt;li>Further optimization of hyperparameters and learning strategy.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prerequisites:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Theoretical knowledge about deep neural networks for computer vision.&lt;/li>
&lt;li>Experience with implementing a deep neural network for computer vision.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Interested?&lt;/strong> Send an email to &lt;a href="mailto:d.c.d.geus@tue.nl">d.c.d.geus@tue.nl&lt;/a>, containing:&lt;/p>
&lt;ul>
&lt;li>Brief motivation letter&lt;/li>
&lt;li>List of relevant courses and grades&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>References&lt;/strong>&lt;/p>
&lt;p>[1] D. de Geus, P. Meletis and G. Dubbelman, &amp;ldquo;Fast Panoptic Segmentation Network&amp;rdquo;, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1742-1749, 2020.&lt;/p></description></item><item><title>An example preprint / working paper</title><link>https://tue-mps.github.io/publication/preprint/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/publication/preprint/</guid><description>&lt;p>Supplementary notes can be added here, including &lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">code and math&lt;/a>.&lt;/p></description></item><item><title>An example journal article</title><link>https://tue-mps.github.io/publication/journal-article/</link><pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/publication/journal-article/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;p>Supplementary notes can be added here, including &lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">code and math&lt;/a>.&lt;/p></description></item><item><title>An example conference paper</title><link>https://tue-mps.github.io/publication/conference-paper/</link><pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/publication/conference-paper/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;p>Supplementary notes can be added here, including &lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">code and math&lt;/a>.&lt;/p></description></item><item><title/><link>https://tue-mps.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/admin/config.yml</guid><description/></item><item><title/><link>https://tue-mps.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/contact/</guid><description/></item><item><title/><link>https://tue-mps.github.io/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tue-mps.github.io/people/</guid><description/></item></channel></rss>